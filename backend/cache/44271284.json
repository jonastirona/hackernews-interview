{"hn_id": "44271284", "title": "Self-Adapting Language Models", "url": "https://news.ycombinator.com/item?id=44271284", "article_url": "https://arxiv.org/abs/2506.10943", "points": 80, "author": "archon1410", "comments_count": 24, "time": 0, "full_article_html": "<main>\n<div id=\"content\">\n<!--\nrdf:RDF xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\"\n         xmlns:dc=\"http://purl.org/dc/elements/1.1/\"\n         xmlns:trackback=\"http://madskills.com/public/xml/rss/module/trackback/\">\n    <rdf:Description\n        rdf:about=\"/abs/2506.10943\"\n        dc:identifier=\"/abs/2506.10943\"\n        dc:title=\"Self-Adapting Language Models\"\n        trackback:ping=\"/trackback/2506.10943\" />\n    </rdf:RDF>\n--><div id=\"abs-outer\">\n<div class=\"leftcolumn\">\n<div class=\"subheader\">\n<h1>Computer Science &gt; Machine Learning</h1>\n</div>\n<div class=\"header-breadcrumbs-mobile\">\n<strong>arXiv:2506.10943</strong> (cs)\n    </div>\n<link href=\"/static/base/1.0.1/css/abs.css\" rel=\"stylesheet\" type=\"text/css\"/>\n<div id=\"content-inner\">\n<div id=\"abs\">\n<div class=\"dateline\">\n  [Submitted on 12 Jun 2025]</div>\n<h1 class=\"title mathjax\"><span class=\"descriptor\">Title:</span>Self-Adapting Language Models</h1>\n<div class=\"authors\"><span class=\"descriptor\">Authors:</span><a href=\"https://arxiv.org/search/cs?searchtype=author&amp;query=Zweiger,+A\" rel=\"nofollow\">Adam Zweiger</a>, <a href=\"https://arxiv.org/search/cs?searchtype=author&amp;query=Pari,+J\" rel=\"nofollow\">Jyothish Pari</a>, <a href=\"https://arxiv.org/search/cs?searchtype=author&amp;query=Guo,+H\" rel=\"nofollow\">Han Guo</a>, <a href=\"https://arxiv.org/search/cs?searchtype=author&amp;query=Aky%C3%BCrek,+E\" rel=\"nofollow\">Ekin Akyürek</a>, <a href=\"https://arxiv.org/search/cs?searchtype=author&amp;query=Kim,+Y\" rel=\"nofollow\">Yoon Kim</a>, <a href=\"https://arxiv.org/search/cs?searchtype=author&amp;query=Agrawal,+P\" rel=\"nofollow\">Pulkit Agrawal</a></div> <div hidden=\"\" id=\"download-button-info\">View a PDF of the paper titled Self-Adapting Language Models, by Adam Zweiger and 5 other authors</div>\n<a class=\"mobile-submission-download\" href=\"https://arxiv.org/pdf/2506.10943\">View PDF</a>\n<a class=\"mobile-submission-download\" href=\"https://arxiv.org/html/2506.10943v1\">HTML (experimental)</a>\n<blockquote class=\"abstract mathjax\">\n<span class=\"descriptor\">Abstract:</span>Large language models (LLMs) are powerful but static; they lack mechanisms to adapt their weights in response to new tasks, knowledge, or examples. We introduce Self-Adapting LLMs (SEAL), a framework that enables LLMs to self-adapt by generating their own finetuning data and update directives. Given a new input, the model produces a self-edit-a generation that may restructure the information in different ways, specify optimization hyperparameters, or invoke tools for data augmentation and gradient-based updates. Through supervised finetuning (SFT), these self-edits result in persistent weight updates, enabling lasting adaptation. To train the model to produce effective self-edits, we use a reinforcement learning loop with the downstream performance of the updated model as the reward signal. Unlike prior approaches that rely on separate adaptation modules or auxiliary networks, SEAL directly uses the model's own generation to control its adaptation process. Experiments on knowledge incorporation and few-shot generalization show that SEAL is a promising step toward language models capable of self-directed adaptation. Our website and code is available at <a class=\"link-external link-https\" href=\"https://jyopari.github.io/posts/seal\" rel=\"external noopener nofollow\">this https URL</a>.\n    </blockquote>\n<!--CONTEXT-->\n<div class=\"metatable\">\n<table summary=\"Additional metadata\"><tbody><tr>\n<td class=\"tablecell label\">Subjects:</td>\n<td class=\"tablecell subjects\">\n<span class=\"primary-subject\">Machine Learning (cs.LG)</span></td>\n</tr><tr>\n<td class=\"tablecell label\">Cite as:</td>\n<td class=\"tablecell arxivid\"><span class=\"arxivid\"><a href=\"https://arxiv.org/abs/2506.10943\">arXiv:2506.10943</a> [cs.LG]</span></td>\n</tr>\n<tr>\n<td class=\"tablecell label\"> </td>\n<td class=\"tablecell arxividv\">(or <span class=\"arxivid\">\n<a href=\"https://arxiv.org/abs/2506.10943v1\">arXiv:2506.10943v1</a> [cs.LG]</span> for this version)\n          </td>\n</tr>\n<tr>\n<td class=\"tablecell label\"> </td>\n<td class=\"tablecell arxivdoi\"> <a href=\"https://doi.org/10.48550/arXiv.2506.10943\" id=\"arxiv-doi-link\">https://doi.org/10.48550/arXiv.2506.10943</a><div class=\"button-and-tooltip\">\n<button aria-describedby=\"more-info-desc-1\" class=\"more-info\">\n<svg height=\"15\" role=\"presentation\" viewbox=\"0 0 512 512\" xmlns=\"http://www.w3.org/2000/svg\"><path class=\"\" d=\"M256 8C119.043 8 8 119.083 8 256c0 136.997 111.043 248 248 248s248-111.003 248-248C504 119.083 392.957 8 256 8zm0 110c23.196 0 42 18.804 42 42s-18.804 42-42 42-42-18.804-42-42 18.804-42 42-42zm56 254c0 6.627-5.373 12-12 12h-88c-6.627 0-12-5.373-12-12v-24c0-6.627 5.373-12 12-12h12v-64h-12c-6.627 0-12-5.373-12-12v-24c0-6.627 5.373-12 12-12h64c6.627 0 12 5.373 12 12v100h12c6.627 0 12 5.373 12 12v24z\" fill=\"currentColor\"></path></svg>\n<span class=\"visually-hidden\">Focus to learn more</span>\n</button>\n<!-- tooltip description -->\n<div id=\"more-info-desc-1\" role=\"tooltip\">\n<span class=\"left-corner\"></span>                  arXiv-issued DOI via DataCite (pending registration)</div>\n</div>\n</td>\n</tr></tbody></table>\n</div>\n</div>\n</div>\n<div class=\"submission-history\">\n<h2>Submission history</h2> From: Jyothish Pari [<a href=\"https://arxiv.org/show-email/3f560f7f/2506.10943\" rel=\"nofollow\">view email</a>]      <br/> <strong>[v1]</strong>\n        Thu, 12 Jun 2025 17:48:13 UTC (1,999 KB)<br/>\n</div>\n</div>\n<!--end leftcolumn-->\n<div class=\"extra-services\"> <div class=\"full-text\">\n<a name=\"other\"></a>\n<span class=\"descriptor\">Full-text links:</span>\n<h2>Access Paper:</h2>\n<ul>\n<div hidden=\"\" id=\"download-button-info\">\nView a PDF of the paper titled Self-Adapting Language Models, by Adam Zweiger and 5 other authors</div><li><a accesskey=\"f\" aria-describedby=\"download-button-info\" class=\"abs-button download-pdf\" href=\"https://arxiv.org/pdf/2506.10943\">View PDF</a></li><li><a class=\"abs-button\" href=\"https://arxiv.org/html/2506.10943v1\" id=\"latexml-download-link\">HTML (experimental)</a></li><li><a class=\"abs-button download-eprint\" href=\"https://arxiv.org/src/2506.10943\">TeX Source</a></li><li><a class=\"abs-button download-format\" href=\"https://arxiv.org/format/2506.10943\">Other Formats</a></li></ul>\n<div class=\"abs-license\"><a class=\"has_license\" href=\"http://creativecommons.org/licenses/by/4.0/\" title=\"Rights to this article\">\n<img alt=\"license icon\" data-src=\"https://arxiv.org/icons/licenses/by-4.0.png\" loading=\"lazy\" onerror=\"this.style.display='none'\" role=\"presentation\" src=\"https://arxiv.org/icons/licenses/by-4.0.png\"/>\n<span>view license</span>\n</a></div>\n</div>\n<!--end full-text--> <div class=\"browse\">\n    Current browse context: <div class=\"current\">cs.LG</div>\n<div class=\"prevnext\">\n<span class=\"arrow\">\n<a accesskey=\"p\" class=\"abs-button prev-url\" href=\"https://arxiv.org/prevnext?id=2506.10943&amp;function=prev&amp;context=cs.LG\" rel=\"nofollow\" title=\"previous in cs.LG (accesskey p)\">&lt; prev</a>\n</span>\n<span class=\"is-hidden-mobile\">  |  </span> <span class=\"arrow\">\n<a accesskey=\"n\" class=\"abs-button next-url\" href=\"https://arxiv.org/prevnext?id=2506.10943&amp;function=next&amp;context=cs.LG\" rel=\"nofollow\" title=\"next in cs.LG (accesskey n)\">next &gt;</a>\n</span><br/>\n</div><div class=\"list\">\n<a class=\"abs-button abs-button-grey abs-button-small context-new\" href=\"https://arxiv.org/list/cs.LG/new\" rel=\"nofollow\">new</a>\n<span class=\"is-hidden-mobile\"> | </span>\n<a class=\"abs-button abs-button-grey abs-button-small context-recent\" href=\"https://arxiv.org/list/cs.LG/recent\" rel=\"nofollow\">recent</a>\n<span class=\"is-hidden-mobile\"> | </span><a class=\"abs-button abs-button-grey abs-button-small context-id\" href=\"https://arxiv.org/list/cs.LG/2025-06\" rel=\"nofollow\">2025-06</a>\n</div><div class=\"abs-switch-cat\">\n    Change to browse by:\n    <div class=\"switch context-change\">\n<a href=\"https://arxiv.org/abs/2506.10943?context=cs\" rel=\"nofollow\">cs</a><br class=\"is-hidden-mobile\"/>\n</div>\n</div>\n</div>\n<div class=\"extra-ref-cite\">\n<h3>References &amp; Citations</h3>\n<ul>\n<li><a class=\"abs-button abs-button-small cite-ads\" href=\"https://ui.adsabs.harvard.edu/abs/arXiv:2506.10943\">NASA ADS</a></li><li><a class=\"abs-button abs-button-small cite-google-scholar\" href=\"https://scholar.google.com/scholar_lookup?arxiv_id=2506.10943\" rel=\"noopener\" target=\"_blank\">Google Scholar</a></li>\n<li><a class=\"abs-button abs-button-small cite-semantic-scholar\" href=\"https://api.semanticscholar.org/arXiv:2506.10943\" rel=\"noopener\" target=\"_blank\">Semantic Scholar</a></li>\n</ul>\n<div style=\"clear:both;\"></div>\n</div>\n<div class=\"extra-ref-cite\">\n<a hidden=\"true\" href=\"https://arxiv.org/static/browse/0.3.4/css/cite.css\" id=\"bib-cite-css\">a</a>\n<span class=\"bib-cite-button abs-button\" id=\"bib-cite-trigger\">export BibTeX citation</span>\n<span hidden=\"true\" id=\"bib-cite-loading\">Loading...</span>\n</div>\n<div class=\"bib-modal\" hidden=\"true\" id=\"bib-cite-modal\">\n<div class=\"bib-modal-content\">\n<div class=\"bib-modal-title\">\n<h2>BibTeX formatted citation</h2>\n<span class=\"bib-modal-close\">×</span>\n</div>\n<div>\n<textarea aria-label=\"loading the citation\" class=\"bib-citation-content\" id=\"bib-cite-target\">loading...</textarea>\n</div>\n<div>\n<span>Data provided by: </span>\n<a id=\"bib-cite-source-api\"></a>\n</div>\n</div>\n</div><div class=\"bookmarks\">\n<div><h3>Bookmark</h3></div><a class=\"abs-button abs-button-grey abs-button-small\" href=\"http://www.bibsonomy.org/BibtexHandler?requTask=upload&amp;url=https://arxiv.org/abs/2506.10943&amp;description=Self-Adapting Language Models\" title=\"Bookmark on BibSonomy\">\n<img alt=\"BibSonomy logo\" data-src=\"https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png\" loading=\"lazy\" onerror=\"this.style.display='none'\" src=\"https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png\"/>\n</a>\n<a class=\"abs-button abs-button-grey abs-button-small\" href=\"https://reddit.com/submit?url=https://arxiv.org/abs/2506.10943&amp;title=Self-Adapting Language Models\" title=\"Bookmark on Reddit\">\n<img alt=\"Reddit logo\" data-src=\"https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png\" loading=\"lazy\" onerror=\"this.style.display='none'\" src=\"https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png\"/>\n</a>\n</div> </div>\n<!--end extra-services-->\n<!-- LABS AREA -->\n<div id=\"labstabs\">\n<div class=\"labstabs\"><input checked=\"checked\" id=\"tabone\" name=\"tabs\" type=\"radio\"/>\n<label for=\"tabone\">Bibliographic Tools</label>\n<div class=\"tab labs-display-bib\">\n<h1>Bibliographic and Citation Tools</h1>\n<div class=\"toggle\">\n<div class=\"columns is-mobile lab-row\">\n<div class=\"column lab-switch\">\n<label class=\"switch\">\n<input class=\"lab-toggle\" data-script-url=\"/static/browse/0.3.4/bibex/bibex.js?20241202\" id=\"bibex-toggle\" type=\"checkbox\"/>\n<span class=\"slider\"></span>\n<span class=\"is-sr-only\">Bibliographic Explorer Toggle</span>\n</label>\n</div>\n<div class=\"column lab-name\">\n<span id=\"label-for-bibex\">Bibliographic Explorer</span> <em>(<a href=\"https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer\">What is the Explorer?</a>)</em>\n</div>\n</div>\n<div class=\"columns is-mobile lab-row\">\n<div class=\"column lab-switch\">\n<label class=\"switch\">\n<input aria-labelledby=\"label-for-connected-papers\" class=\"lab-toggle\" data-script-url=\"/static/browse/0.3.4/js/connectedpapers.js\" id=\"connectedpapers-toggle\" type=\"checkbox\"/>\n<span class=\"slider\"></span>\n<span class=\"is-sr-only\">Connected Papers Toggle</span>\n</label>\n</div>\n<div class=\"column lab-name\">\n<span id=\"label-for-connected-papers\">Connected Papers</span> <em>(<a href=\"https://www.connectedpapers.com/about\" target=\"_blank\">What is Connected Papers?</a>)</em>\n</div>\n</div><div class=\"columns is-mobile lab-row\">\n<div class=\"column lab-switch\">\n<label class=\"switch\">\n<input aria-labelledby=\"label-for-litmaps\" class=\"lab-toggle\" data-script-url=\"/static/browse/0.3.4/js/litmaps.js?20210617\" id=\"litmaps-toggle\" type=\"checkbox\"/>\n<span class=\"slider\"></span>\n<span class=\"is-sr-only\">Litmaps Toggle</span>\n</label>\n</div>\n<div class=\"column lab-name\">\n<span id=\"label-for-litmaps\">Litmaps</span> <em>(<a href=\"https://www.litmaps.co/\" target=\"_blank\">What is Litmaps?</a>)</em>\n</div>\n</div>\n<div class=\"columns is-mobile lab-row\">\n<div class=\"column lab-switch\">\n<label class=\"switch\">\n<input aria-labelledby=\"label-for-scite\" class=\"lab-toggle\" data-script-url=\"/static/browse/0.3.4/js/scite.js?20210617\" id=\"scite-toggle\" type=\"checkbox\"/>\n<span class=\"slider\"></span>\n<span class=\"is-sr-only\">scite.ai Toggle</span>\n</label>\n</div>\n<div class=\"column lab-name\">\n<span id=\"label-for-scite\">scite Smart Citations</span> <em>(<a href=\"https://www.scite.ai/\" target=\"_blank\">What are Smart Citations?</a>)</em>\n</div>\n</div>\n</div>\n<div class=\"labs-content-placeholder labs-display\" style=\"display: none;\"></div>\n<div id=\"connectedpapers-output\" style=\"min-height: 15px\"></div>\n<div id=\"litmaps-open-in\" style=\"min-height: 15px\"></div>\n<div id=\"scite-open-in\" style=\"min-height: 15px\"></div>\n</div>\n<input id=\"tabtwo\" name=\"tabs\" type=\"radio\"/>\n<label for=\"tabtwo\">Code, Data, Media</label>\n<div class=\"tab\">\n<h1>Code, Data and Media Associated with this Article</h1>\n<div class=\"toggle\">\n<div class=\"columns is-mobile lab-row\">\n<div class=\"column lab-switch\">\n<label class=\"switch\">\n<input aria-labelledby=\"label-for-alphaxiv\" class=\"lab-toggle\" data-script-url=\"/static/browse/0.3.4/js/alphaxiv.js\" id=\"alphaxiv-toggle\" type=\"checkbox\"/>\n<span class=\"slider\"></span>\n<span class=\"is-sr-only\">alphaXiv Toggle</span>\n</label>\n</div>\n<div class=\"column lab-name\">\n<span id=\"label-for-alphaxiv\">alphaXiv</span> <em>(<a href=\"https://alphaxiv.org/\" target=\"_blank\">What is alphaXiv?</a>)</em>\n</div>\n</div>\n<div class=\"columns is-mobile lab-row\">\n<div class=\"column lab-switch\">\n<label class=\"switch\">\n<input aria-labelledby=\"label-for-cx\" class=\"lab-toggle\" data-script-url=\"/static/browse/0.3.4/js/catalyzex.js\" id=\"catalyzex-toggle\" type=\"checkbox\"/>\n<span class=\"slider\"></span>\n<span class=\"is-sr-only\">Links to Code Toggle</span>\n</label>\n</div>\n<div class=\"column lab-name\">\n<span id=\"label-for-cx\">CatalyzeX Code Finder for Papers</span> <em>(<a href=\"https://www.catalyzex.com\" target=\"_blank\">What is CatalyzeX?</a>)</em>\n</div>\n</div>\n<div class=\"columns is-mobile lab-row\">\n<div class=\"column lab-switch\">\n<label class=\"switch\">\n<input aria-labelledby=\"label-for-dagshub\" class=\"lab-toggle\" data-script-url=\"/static/browse/0.3.4/js/dagshub.js\" id=\"dagshub-toggle\" type=\"checkbox\"/>\n<span class=\"slider\"></span>\n<span class=\"is-sr-only\">DagsHub Toggle</span>\n</label>\n</div>\n<div class=\"column lab-name\">\n<span id=\"label-for-dagshub\">DagsHub</span> <em>(<a href=\"https://dagshub.com/\" target=\"_blank\">What is DagsHub?</a>)</em>\n</div>\n</div>\n<div class=\"columns is-mobile lab-row\">\n<div class=\"column lab-switch\">\n<label class=\"switch\">\n<input aria-labelledby=\"label-for-gotitpub\" class=\"lab-toggle\" data-script-url=\"/static/browse/0.3.4/js/gotitpub.js\" id=\"gotitpub-toggle\" type=\"checkbox\"/>\n<span class=\"slider\"></span>\n<span class=\"is-sr-only\">GotitPub Toggle</span>\n</label>\n</div>\n<div class=\"column lab-name\">\n<span id=\"label-for-gotitpub\">Gotit.pub</span> <em>(<a href=\"http://gotit.pub/faq\" target=\"_blank\">What is GotitPub?</a>)</em>\n</div>\n</div>\n<div class=\"columns is-mobile lab-row\">\n<div class=\"column lab-switch\">\n<label class=\"switch\">\n<input aria-labelledby=\"label-for-huggingface\" class=\"lab-toggle\" data-script-url=\"/static/browse/0.3.4/js/huggingface.js\" id=\"huggingface-toggle\" type=\"checkbox\"/>\n<span class=\"slider\"></span>\n<span class=\"is-sr-only\">Huggingface Toggle</span>\n</label>\n</div>\n<div class=\"column lab-name\">\n<span id=\"label-for-huggingface\">Hugging Face</span> <em>(<a href=\"https://huggingface.co/huggingface\" target=\"_blank\">What is Huggingface?</a>)</em>\n</div>\n</div>\n<div class=\"columns is-mobile lab-row\">\n<div class=\"column lab-switch\">\n<label class=\"switch\">\n<input aria-labelledby=\"label-for-pwc\" class=\"lab-toggle\" data-script-url=\"/static/browse/0.3.4/js/paperswithcode.js\" id=\"paperwithcode-toggle\" type=\"checkbox\"/>\n<span class=\"slider\"></span>\n<span class=\"is-sr-only\">Links to Code Toggle</span>\n</label>\n</div>\n<div class=\"column lab-name\">\n<span id=\"label-for-pwc\">Papers with Code</span> <em>(<a href=\"https://paperswithcode.com/\" target=\"_blank\">What is Papers with Code?</a>)</em>\n</div>\n</div>\n<div class=\"columns is-mobile lab-row\">\n<div class=\"column lab-switch\">\n<label class=\"switch\">\n<input aria-labelledby=\"label-for-sciencecast\" class=\"lab-toggle\" data-script-url=\"/static/browse/0.3.4/js/sciencecast.js\" id=\"sciencecast-toggle\" type=\"checkbox\"/>\n<span class=\"slider\"></span>\n<span class=\"is-sr-only\">ScienceCast Toggle</span>\n</label>\n</div>\n<div class=\"column lab-name\">\n<span id=\"label-for-sciencecast\">ScienceCast</span> <em>(<a href=\"https://sciencecast.org/welcome\" target=\"_blank\">What is ScienceCast?</a>)</em>\n</div>\n</div>\n</div>\n<div id=\"alphaxiv-output\" style=\"display:none\"></div>\n<div id=\"catalyzex-output\" style=\"display:none\"></div>\n<div id=\"dagshub-output\" style=\"display:none\"></div>\n<div id=\"gotitpub-output\" style=\"display:none\"></div>\n<div id=\"pwc-output\" style=\"display:none\"></div>\n<div id=\"pwc-data-output\" style=\"display:none\"></div>\n<div id=\"sciencecast-output\" style=\"display:none\"></div>\n<div id=\"huggingface-output\" style=\"display:none\"></div>\n</div>\n<input id=\"labstabs-demos-input\" name=\"tabs\" type=\"radio\"/>\n<label for=\"labstabs-demos-input\" id=\"labstabs-demos-label\">Demos</label>\n<div class=\"tab\">\n<h1>Demos</h1>\n<div class=\"toggle\">\n<div class=\"columns is-mobile lab-row\">\n<div class=\"column lab-switch\">\n<label class=\"switch\">\n<input aria-labelledby=\"label-for-replicate\" class=\"lab-toggle\" data-script-url=\"/static/browse/0.3.4/js/replicate.js\" id=\"replicate-toggle\" type=\"checkbox\"/>\n<span class=\"slider\"></span>\n<span class=\"is-sr-only\">Replicate Toggle</span>\n</label>\n</div>\n<div class=\"column lab-name\">\n<span id=\"label-for-replicate\">Replicate</span> <em>(<a href=\"https://replicate.com/docs/arxiv/about\" target=\"_blank\">What is Replicate?</a>)</em>\n</div>\n</div>\n<div class=\"columns is-mobile lab-row\">\n<div class=\"column lab-switch\">\n<label class=\"switch\">\n<input aria-labelledby=\"label-for-spaces\" class=\"lab-toggle\" data-script-url=\"/static/browse/0.3.4/js/spaces.js\" id=\"spaces-toggle\" type=\"checkbox\"/>\n<span class=\"slider\"></span>\n<span class=\"is-sr-only\">Spaces Toggle</span>\n</label>\n</div>\n<div class=\"column lab-name\">\n<span id=\"label-for-spaces\">Hugging Face Spaces</span> <em>(<a href=\"https://huggingface.co/docs/hub/spaces\" target=\"_blank\">What is Spaces?</a>)</em>\n</div>\n</div>\n<div class=\"columns is-mobile lab-row\">\n<div class=\"column lab-switch\">\n<label class=\"switch\">\n<input aria-labelledby=\"label-for-txyz\" class=\"lab-toggle\" data-script-url=\"/static/browse/0.3.4/js/txyz.js\" id=\"txyz-toggle\" type=\"checkbox\"/>\n<span class=\"slider\"></span>\n<span class=\"is-sr-only\">Spaces Toggle</span>\n</label>\n</div>\n<div class=\"column lab-name\">\n<span id=\"label-for-txyz\">TXYZ.AI</span> <em>(<a href=\"https://txyz.ai\" target=\"_blank\">What is TXYZ.AI?</a>)</em>\n</div>\n</div>\n</div>\n<div id=\"replicate-output\"></div>\n<div id=\"spaces-output\"></div>\n<div id=\"txyz-output\"></div>\n</div>\n<input id=\"tabfour\" name=\"tabs\" type=\"radio\"/>\n<label for=\"tabfour\">Related Papers</label>\n<div class=\"tab\">\n<h1>Recommenders and Search Tools</h1>\n<div class=\"toggle\">\n<div class=\"columns is-mobile lab-row\">\n<div class=\"column lab-switch\">\n<label class=\"switch\">\n<input aria-labelledby=\"label-for-influenceflower\" class=\"lab-toggle\" data-script-url=\"/static/browse/0.3.4/js/influenceflower.js\" id=\"influenceflower-toggle\" type=\"checkbox\"/>\n<span class=\"slider\"></span>\n<span class=\"is-sr-only\">Link to Influence Flower</span>\n</label>\n</div>\n<div class=\"column lab-name\">\n<span id=\"label-for-influenceflower\">Influence Flower</span> <em>(<a href=\"https://influencemap.cmlab.dev/\" target=\"_blank\">What are Influence Flowers?</a>)</em>\n</div>\n</div>\n<div class=\"columns is-mobile lab-row\">\n<div class=\"column lab-switch\">\n<label class=\"switch\">\n<input aria-labelledby=\"label-for-core\" class=\"lab-toggle\" id=\"core-recommender-toggle\" type=\"checkbox\"/>\n<span class=\"slider\"></span>\n<span class=\"is-sr-only\">Core recommender toggle</span>\n</label>\n</div>\n<div class=\"column lab-name\">\n<span id=\"label-for-core\">CORE Recommender</span> <em>(<a href=\"https://core.ac.uk/services/recommender\">What is CORE?</a>)</em>\n</div>\n</div> <div class=\"columns is-mobile lab-row\">\n<div class=\"column lab-switch\">\n<label class=\"switch\">\n<input aria-labelledby=\"label-for-iarxiv\" class=\"lab-toggle\" data-script-url=\"/static/browse/0.3.4/js/iarxiv.js\" id=\"iarxiv-toggle\" type=\"checkbox\"/>\n<span class=\"slider\"></span>\n<span class=\"is-sr-only\">IArxiv recommender toggle</span>\n</label>\n</div>\n<div class=\"column lab-name\">\n<span id=\"label-for-iarxiv\">IArxiv Recommender</span>\n<em>(<a href=\"https://iarxiv.org/about\">What is IArxiv?</a>)</em>\n</div>\n</div>\n</div>\n<div id=\"influenceflower-output\"></div>\n<div id=\"influenceflower-output-graph\" style=\"display:none\">\n<ul class=\"flower-tabs\">\n<li class=\"active\"><a class=\"btn tab-btn\" onclick=\"openTab(event, 'tab-author')\">Author</a></li>\n<li><a class=\"btn tab-btn\" onclick=\"openTab(event, 'tab-venue')\">Venue</a></li>\n<li><a class=\"btn tab-btn\" onclick=\"openTab(event, 'tab-inst')\">Institution</a></li>\n<li><a class=\"btn tab-btn\" onclick=\"openTab(event, 'tab-topic')\">Topic</a></li>\n</ul>\n<div class=\"flower-tab-content\">\n<div class=\"tab-flower active\" id=\"tab-author\"><svg id=\"flower-graph-author\"></svg></div>\n<div class=\"tab-flower\" id=\"tab-venue\"><svg id=\"flower-graph-venue\"></svg></div>\n<div class=\"tab-flower\" id=\"tab-inst\"><svg id=\"flower-graph-inst\"></svg></div>\n<div class=\"tab-flower\" id=\"tab-topic\"><svg id=\"flower-graph-topic\"></svg></div>\n</div>\n</div>\n<div id=\"coreRecommenderOutput\"></div>\n<div id=\"iarxivOutput\"></div>\n</div>\n<input id=\"tabfive\" name=\"tabs\" type=\"radio\"/>\n<label for=\"tabfive\">\n        About arXivLabs\n      </label>\n<div class=\"tab\">\n<div class=\"columns\">\n<div class=\"column\">\n<h1>arXivLabs: experimental projects with community collaborators</h1>\n<p>arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.</p>\n<p>Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.</p>\n<p>Have an idea for a project that will add value for arXiv's community? <a href=\"https://info.arxiv.org/labs/index.html\"><strong>Learn more about arXivLabs</strong></a>.</p>\n</div>\n<div class=\"column is-narrow is-full-mobile\">\n<p class=\"icon-labs\"><svg role=\"presentation\" viewbox=\"0 0 635.572 811\" xmlns=\"http://www.w3.org/2000/svg\"><path d=\"M175.6 676v27h-27v-27zm-54 27v27h27v-27zm-27 27v27h27v-27zm396-54v27h-27v-27zm0 27v27h27v-27zm27 27v27h27v-27zm-27-414h27v27h-27zm27 0h27v-27h-27zm27-27h27v-27h-27zm-396 45h-27v-27h27zm-27-54h-27v27h27zm-27-27h-27v27h27z\"></path><path d=\"M94.6 730v27h-27v-27zm477 0v27h-27v-27zm-27-495h27v27h-27zm-450 18h-27v-27h27zm477 9h27v27h-27zm-54 495h27v27h-27zm-423 0h27v27h-27zm-54-504h27v27h-27z\" fill=\"#666\"></path><path d=\"M67.6 730v27h-27v-27zm54 54v27h-27v-27zm0-108v27h27v-27zm-27 27v27h27v-27zm-81 0v27h27v-27zm585 27v27h-27v-27zm-108-54v27h27v-27zm27 27v27h27v-27zm81 0v27h27v-27zm-54-495h27v27h-27zm-54 108h27v-27h-27zm27-27h27v-27h-27zm0-81h27v-27h-27zm-423 18h-27v-27h27zm54 54h-27v27h27zm-27-27h-27v27h27zm0-81h-27v27h27zm423 612v27h-27v-27zm81-522v27h-27v-27zm-585-9v27h-27v-27z\" fill=\"#999\"></path><path d=\"M94.6 784v27h-27v-27zm-27-27v27h27v-27zm-27-54v27h27v-27zm27 0v27h27v-27zm0-27v27h27v-27zm27 0v27h27v-27zm0-27v27h27v-27zm27 0v27h27v-27zm-108 81v27h27v-27zm558 54v27h-27v-27zm-27-27v27h27v-27zm27-54v27h27v-27zm-27 0v27h27v-27zm0-27v27h27v-27zm-27 0v27h27v-27zm0-27v27h27v-27zm-27 0v27h27v-27zm108 81v27h27v-27zm0-495h27v27h-27zm-27 27h27v-27h-27zm-54-27h27v-27h-27zm0 27h27v-27h-27zm-27 0h27v-27h-27zm0 27h27v-27h-27zm-27 0h27v-27h-27zm0 27h27v-27h-27zm81-108h27v-27h-27zm-504 45h-27v-27h27zm27-27h-27v27h27zm54-27h-27v27h27zm0 27h-27v27h27zm27 0h-27v27h27zm0 27h-27v27h27zm27 0h-27v27h27zm0 27h-27v27h27zm-81-108h-27v27h27z\" fill=\"#ccc\"></path><path d=\"M598.6 665.1H41.5C-76.5 667 176 280.2 176 280.2h53a46.5 46.5 0 0162.8-56.3 29.2 29.2 0 1128.5 35.9h-1a46.5 46.5 0 01-1.5 20.3l142.5-.1s255.3 387 138.3 385.1zM291 181a29.3 29.3 0 10-29.2-29.3A29.3 29.3 0 00291 181zm65.4-66.8a22.4 22.4 0 10-22.5-22.4 22.4 22.4 0 0022.5 22.4z\" fill=\"#fc0\"></path><path d=\"M245.5 172V10h153v162s324 495 198 495h-558c-126 0 207-495 207-495zm126 54h56m-13 72h56m-9 72h56m-20 72h56m-22 72h56m-29 72h56m-457-45c20.8 41.7 87.3 81 160.7 81 72.1 0 142.1-38.2 163.4-81\" fill=\"none\" stroke=\"#000\" stroke-miterlimit=\"10\" stroke-width=\"20\"></path><path d=\"M273.3 421.7c0 31-9.8 56.3-21.9 56.3s-21.8-25.2-21.8-56.3 9.8-56.3 21.8-56.3 21.9 25.2 21.9 56.3zm114.4-56.3c-12 0-21.8 25.2-21.8 56.3s9.7 56.3 21.8 56.3 21.9-25.2 21.9-56.3-9.8-56.3-21.9-56.3zM150.1 526.6c-18.2 6.7-27.5 22.9-23.2 30.2s14.8-5.5 33-12.2 37.4-4.9 33-12.2-24.5-12.6-42.8-5.8zm296 5.8c-4.2 7.3 14.9 5.5 33.1 12.2s28.7 19.5 33 12.2-5-23.5-23.2-30.2-38.5-1.5-42.8 5.8z\"></path></svg></p>\n</div>\n</div>\n</div>\n</div>\n</div>\n<!-- END LABS AREA -->\n<div class=\"endorsers\">\n<a class=\"endorser-who\" href=\"https://arxiv.org/auth/show-endorsers/2506.10943\" rel=\"nofollow\">Which authors of this paper are endorsers?</a> |\n    <a href=\"javascript:setMathjaxCookie()\" id=\"mathjax_toggle\">Disable MathJax</a> (<a href=\"https://info.arxiv.org/help/mathjax.html\">What is MathJax?</a>)\n    <span class=\"help\" style=\"font-style: normal; float: right; margin-top: 0; margin-right: 1em;\"></span>\n</div>\n<script language=\"javascript\" type=\"text/javascript\">mathjaxToggle();</script>\n</div>\n</div>\n</main>", "article_metadata": {"title": "[2506.10943] Self-Adapting Language Models", "description": "Abstract page for arXiv paper 2506.10943: Self-Adapting Language Models", "og_image": "/static/browse/0.3.4/images/arxiv-logo-fb.png"}, "screenshot_path": "/static/screenshots/44271284.png", "screenshot_error": null, "hook": "There was an error processing this article. Please click the link to read more.", "top_comments": [{"author": "xianshou", "text": "The self-edit approach is clever - using RL to optimize how models restructure information for their own learning. The key insight is that different representations work better for different types of knowledge, just like how humans take notes differently for math vs history.\n\nTwo things that stand out:\n\n- The knowledge incorporation results (47% vs 46.3% with GPT-4.1 data, both much higher than the small-model baseline) show the model does discover better training formats, not just more data. Though the catastrophic forgetting problem remains unsolved, and it's not completely clear whether data diversity is improved.\n\n- The computational overhead is brutal - 30-45 seconds per reward evaluation makes this impractical for most use cases. But for high-value document processing where you really need optimal retention, it could be worth it.\n\nThe restriction to tasks with explicit evaluation metrics is the main limitation. You need ground truth Q&A pairs or test cases to compute rewards. Still, for domains like technical documentation or educational content where you can generate evaluations, this could significantly improve how we process new information.\n\nFeels like an important step toward models that can adapt their own learning strategies, even if we're not quite at the \"continuously self-improving agent\" stage yet.\n\nreply", "depth": 0}, {"author": "cma", "text": "From Anthropic a couple days ago too, self finetuning:\n\nhttps://arxiv.org/html/2506.10139v1\n\nreply", "depth": 0}, {"author": "Centigonal", "text": "It seems to me that \"forgetting correctly\" is rapidly becoming a more pertinent problem in this field than \"learning correctly.\" We're making great strides in getting models to teach themselves new facts, but the state of the art in jettisoning the least relevant information given new knowledge and finite capacity is lagging far behind.\n\n\"Forgetting correctly\" is something most human brains are exceptionally good at, too. I wonder how that works...\n\nreply", "depth": 0}, {"author": "johnsmith1840", "text": "Did an interesting study that actually LLMs \"hide\" internal data.\n\nThey don't just \"forget\" that information can come back at a later time if you continue to train.\n\nSo basically any time a model is trained you need to check it's entire memory not just a small part.\n\nreply", "depth": 1}, {"author": "campbel", "text": "Is it some form of least-recently-used approach? I'm running tests on my own mind trying to figure it out now :D part of what I love about this area of computer science.\n\nreply", "depth": 1}, {"author": "libraryofbabel", "text": "I wonder if anyone who’s really in the know could summarize where the research is at with getting LLMs to learn “on the job” (through continuous fine tuning or whatever) and what the blockers are to this being a useful deployable thing, e.g. having a model+coding agent that can actually learn a codebase over time (cost? model collapse? something else?).\n\nI’m sure this is something the big labs are trying but from the outside as a user of LLMs it feels like people don’t talk about this very much and instead the focus right now is on better training (eg reinforcement learning) with the assumption that anything else not learned during training will be stuffed into the context somehow as needed. But from a naive perspective the lack of learning from experience after training seems like the biggest thing standing between us and AGI.\n\nreply", "depth": 0}, {"author": "johnsmith1840", "text": "We have no idea how to do continual learning.\n\nMany people here are right, compute, collapse, forgetting whatever.\n\nThe only \"real\" way to do this would be: 1. Train a model 2. New data 3. Retrain the model in full + new data 4. Repeat 5. You still have no garuntee on the \"time\" aspect though.\n\nBut CL as a field basically has zero answers on how to do this in a true sense. It's crazy hard because the \"solutions\" are hypocritical in many ways.\n\nWe need to expand the model's representation space while keeping the previous representation space nearly the same?\n\nBasically, you need to modify it without changing it.\n\nMost annoying is that even the smallest of natural brains do this easily. I have a long winded theory but basically it boils down to AI likely needs to \"sleep\" or rest somehow.\n\nreply", "depth": 1}, {"author": "kcorbitt", "text": "The real answer is that nobody trusts their automated evals enough to be confident that any given automatically-trained release actually improves performance, even if eval scores go up. So for now everyone batches up updates and vibe-checks them before rolling them out.\n\nreply", "depth": 1}, {"author": "mnahkies", "text": "I'm no expert, but I'd imagine privacy plays (or should play) a big role in this. I'd expect that compute costs mean any learning would have to be in aggregate rather than specific to the user which would then risk leaking information across sessions very likely.\n\nI completely agree that figuring out a safe way to continually train feels like the biggest blocker to AGI\n\nreply", "depth": 1}, {"author": "free_bip", "text": "The most obvious problem is alignment. LLM finetuning is already known to be able to get rid of alignment, so any form of continuous fine tuning would in theory be able to as well.\n\nreply", "depth": 1}], "analysis": {"analysis": "Error analyzing article content.", "metadata": {"error": "400 API key expired. Please renew the API key. [reason: \"API_KEY_INVALID\"\ndomain: \"googleapis.com\"\nmetadata {\n  key: \"service\"\n  value: \"generativelanguage.googleapis.com\"\n}\n, locale: \"en-US\"\nmessage: \"API key expired. Please renew the API key.\"\n]", "model": "gemini-1.5-flash"}}, "has_more": true}